#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
pt_chunks_to_hf_parquet.py

Convert torch-saved *.pt chunks (generated by generate_robust_hp_fast.py) into
HuggingFace-friendly Parquet shards, streaming chunk-by-chunk to stay within RAM.

Key features:
- Reads ONE .pt chunk at a time (list[dict]) -> writes parquet parts.
- Drops any sample where ANY required field is None (or missing), counts and reports.
- Stores large tensors as (bytes + shape) to avoid huge Python object overhead:
    emb_diff/audio_features/clean_audio_features

Example:
  python pt_chunks_to_hf_parquet.py \
    --pt_dir data/dump/train_snr0 \
    --out_dir data/hf_parquet/train_snr0 \
    --shard_size 2000 \
    --float_dtype float16 \
    --max_input_length 1024 \
    --bad_log data/hf_parquet/train_snr0_skipped.jsonl
"""

from __future__ import annotations

import argparse
import gc
import glob
import json
import os
import re
from collections import Counter
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import torch

try:
    import pyarrow as pa
    import pyarrow.parquet as pq
except Exception as e:  # pragma: no cover
    raise SystemExit(
        "Missing dependency 'pyarrow'. Install it first, e.g.:\n"
        "  pip install pyarrow\n\n"
        f"Original import error: {e}"
    )


def _chunk_sort_key(path: str) -> Tuple[int, str]:
    """Sort by chunkNNNNNN if present, else lexicographically."""
    base = os.path.basename(path)
    m = re.search(r"chunk(\d+)", base)
    if m:
        return (int(m.group(1)), base)
    return (10**18, base)


def list_pt_files(pt_dir: str, recursive: bool = False) -> List[str]:
    pattern = "**/*.pt" if recursive else "*.pt"
    paths = glob.glob(os.path.join(pt_dir, pattern), recursive=recursive)
    paths = [p for p in paths if os.path.isfile(p)]
    paths = sorted(paths, key=_chunk_sort_key)
    if not paths:
        raise FileNotFoundError(f"No *.pt found under: {pt_dir}")
    return paths


def _tensor_to_bytes_and_shape(t: torch.Tensor, float_dtype: str) -> Tuple[bytes, List[int]]:
    if t.dtype.is_floating_point:
        t = t.to(getattr(torch, float_dtype))
    t = t.contiguous().cpu()
    arr = t.numpy()
    return arr.tobytes(), list(arr.shape)


def _to_list_int(t: torch.Tensor, max_len: int) -> List[int]:
    t = t[:max_len].to(torch.int32).contiguous().cpu()
    return t.tolist()


def _to_list_float(x: Any) -> List[float]:
    if isinstance(x, (list, tuple)):
        return [float(v) for v in x]
    if isinstance(x, np.ndarray):
        return [float(v) for v in x.tolist()]
    raise TypeError(f"am_score not list/tuple/ndarray: {type(x)}")


def _to_list_str(x: Any) -> List[str]:
    if isinstance(x, (list, tuple)):
        return [str(v) for v in x]
    raise TypeError(f"input not list/tuple: {type(x)}")


REQUIRED_KEYS = (
    "id",
    "input_ids",
    "input_ids_no_response",
    "labels",
    "input",
    "ground_truth",
    "am_score",
    "emb_diff",
    "audio_features",
    "clean_audio_features",
)


def validate_sample(s: Any) -> Tuple[bool, str, List[str]]:
    """Return (ok, reason, none_keys). Drop if any required key missing/None."""
    if s is None:
        return False, "sample_is_none", ["<sample>"]
    if not isinstance(s, dict):
        return False, f"sample_not_dict:{type(s)}", ["<sample>"]

    none_keys: List[str] = []
    for k in REQUIRED_KEYS:
        if k not in s:
            return False, f"missing_key:{k}", [k]
        if s[k] is None:
            none_keys.append(k)
    if none_keys:
        return False, "none_value", none_keys

    # List-valued fields: drop if they contain None elements
    if isinstance(s.get("input"), (list, tuple)) and any(x is None for x in s["input"]):
        return False, "none_value", ["input"]
    if isinstance(s.get("am_score"), (list, tuple)) and any(x is None for x in s["am_score"]):
        return False, "none_value", ["am_score"]

    # Type sanity
    if not torch.is_tensor(s["input_ids"]) or not torch.is_tensor(s["labels"]) or not torch.is_tensor(s["input_ids_no_response"]):
        return False, "input_ids_or_labels_not_tensor", []
    if not torch.is_tensor(s["emb_diff"]):
        return False, "emb_diff_not_tensor", []
    if not torch.is_tensor(s["audio_features"]) or not torch.is_tensor(s["clean_audio_features"]):
        return False, "audio_features_not_tensor", []
    if not isinstance(s["ground_truth"], str):
        return False, "ground_truth_not_str", []
    if not isinstance(s.get("input"), (list, tuple)):
        return False, "input_not_list", []
    if not isinstance(s.get("am_score"), (list, tuple, np.ndarray)):
        return False, "am_score_not_list", []

    return True, "", []


def make_schema() -> pa.Schema:
    return pa.schema(
        [
            ("id", pa.string()),
            ("input_ids", pa.list_(pa.int32())),
            ("input_ids_no_response", pa.list_(pa.int32())),
            ("labels", pa.list_(pa.int32())),
            ("input", pa.list_(pa.string())),
            ("ground_truth", pa.string()),
            ("am_score", pa.list_(pa.float32())),
            ("emb_diff", pa.large_binary()),
            ("emb_diff_shape", pa.list_(pa.int32())),
            ("audio_features", pa.large_binary()),
            ("audio_features_shape", pa.list_(pa.int32())),
            ("clean_audio_features", pa.large_binary()),
            ("clean_audio_features_shape", pa.list_(pa.int32())),
        ]
    )


def write_jsonl(path: str, obj: Dict[str, Any]) -> None:
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")


def flush_parquet(writer: pq.ParquetWriter, buf: Dict[str, List[Any]], schema: pa.Schema) -> None:
    # Avoid creating empty tables
    any_len = len(next(iter(buf.values()))) if buf else 0
    if any_len == 0:
        return
    table = pa.Table.from_pydict(buf, schema=schema)
    writer.write_table(table)


def convert(
    pt_dir: str,
    out_dir: str,
    shard_size: int,
    float_dtype: str,
    max_input_length: int,
    bad_log: Optional[str],
    compression: str,
    recursive: bool,
) -> None:
    os.makedirs(out_dir, exist_ok=True)
    pt_files = list_pt_files(pt_dir, recursive=recursive)
    schema = make_schema()

    def new_writer(part_id: int) -> pq.ParquetWriter:
        out_path = os.path.join(out_dir, f"part-{part_id:05d}.parquet")
        return pq.ParquetWriter(
            out_path,
            schema=schema,
            compression=None if compression == "none" else compression,
        )

    part_id = 0
    writer = new_writer(part_id)

    buf: Dict[str, List[Any]] = {name: [] for name in schema.names}
    n_in_part = 0

    # Counters
    n_seen = 0
    n_kept = 0
    n_dropped_none = 0
    none_key_counter: Counter[str] = Counter()
    n_dropped_missing = 0
    n_dropped_type = 0
    n_dropped_serialize = 0

    for pt_path in pt_files:
        print(f"[load] {pt_path}")
        data = torch.load(pt_path, map_location="cpu")
        if not isinstance(data, list):
            raise TypeError(f"Expect list in {pt_path}, got {type(data)}")

        for i, s in enumerate(data):
            n_seen += 1
            ok, reason, none_keys = validate_sample(s)
            if not ok:
                if reason.startswith("missing_key:"):
                    n_dropped_missing += 1
                elif reason == "none_value":
                    n_dropped_none += 1
                    for k in none_keys:
                        none_key_counter[k] += 1
                else:
                    n_dropped_type += 1

                if bad_log:
                    write_jsonl(
                        bad_log,
                        {
                            "pt": pt_path,
                            "index_in_chunk": i,
                            "id": (s.get("id") if isinstance(s, dict) else None),
                            "reason": reason,
                            "none_keys": none_keys,
                        },
                    )
                continue

            # Serialize / convert
            try:
                sid = str(s["id"])

                input_ids = _to_list_int(s["input_ids"], max_input_length)
                input_ids_no_resp = _to_list_int(s["input_ids_no_response"], max_input_length)
                labels = _to_list_int(s["labels"], max_input_length)

                inp_list = _to_list_str(s["input"])
                gt = str(s["ground_truth"])
                am_score = _to_list_float(s["am_score"])

                emb_b, emb_sh = _tensor_to_bytes_and_shape(s["emb_diff"], float_dtype)
                af_b, af_sh = _tensor_to_bytes_and_shape(s["audio_features"], float_dtype)
                caf_b, caf_sh = _tensor_to_bytes_and_shape(s["clean_audio_features"], float_dtype)

                buf["id"].append(sid)
                buf["input_ids"].append(input_ids)
                buf["input_ids_no_response"].append(input_ids_no_resp)
                buf["labels"].append(labels)
                buf["input"].append(inp_list)
                buf["ground_truth"].append(gt)
                buf["am_score"].append([float(v) for v in am_score])
                buf["emb_diff"].append(emb_b)
                buf["emb_diff_shape"].append([int(x) for x in emb_sh])
                buf["audio_features"].append(af_b)
                buf["audio_features_shape"].append([int(x) for x in af_sh])
                buf["clean_audio_features"].append(caf_b)
                buf["clean_audio_features_shape"].append([int(x) for x in caf_sh])

                n_in_part += 1
                n_kept += 1
            except Exception as e:
                n_dropped_serialize += 1
                if bad_log:
                    write_jsonl(
                        bad_log,
                        {
                            "pt": pt_path,
                            "index_in_chunk": i,
                            "id": (s.get("id") if isinstance(s, dict) else None),
                            "reason": f"serialize_error:{type(e).__name__}:{str(e)}",
                        },
                    )
                continue

            if n_in_part >= shard_size:
                flush_parquet(writer, buf, schema)
                for k in buf:
                    buf[k].clear()
                writer.close()
                part_id += 1
                writer = new_writer(part_id)
                n_in_part = 0

        del data
        gc.collect()

    # final flush
    if n_in_part > 0:
        flush_parquet(writer, buf, schema)
    writer.close()

    # Summary
    print("\n==== Conversion summary ====")
    print(f"seen_total          : {n_seen}")
    print(f"kept_total          : {n_kept}")
    print(f"dropped_missing_key : {n_dropped_missing}")
    print(f"dropped_any_none    : {n_dropped_none}")
    if n_dropped_none > 0:
        print("-- none field counts (key -> count) --")
        for k, v in none_key_counter.most_common():
            print(f"  {k}: {v}")
    print(f"dropped_type        : {n_dropped_type}")
    print(f"dropped_serialize   : {n_dropped_serialize}")
    print(f"out_dir             : {out_dir}")


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--pt_dir", required=True, help="Folder containing *.pt chunks")
    ap.add_argument("--out_dir", required=True, help="Output folder to write parquet parts")
    ap.add_argument("--shard_size", type=int, default=2000, help="Rows per output parquet file")
    ap.add_argument("--float_dtype", choices=["float16", "float32"], default="float16")
    ap.add_argument("--max_input_length", type=int, default=1024, help="Truncate input_ids/labels to this length")
    ap.add_argument("--bad_log", default=None, help="Optional JSONL file to log dropped samples")
    ap.add_argument("--compression", choices=["zstd", "snappy", "gzip", "none"], default="zstd")
    ap.add_argument("--recursive", action="store_true", help="Recursively search for *.pt under pt_dir")
    args = ap.parse_args()

    convert(
        pt_dir=args.pt_dir,
        out_dir=args.out_dir,
        shard_size=args.shard_size,
        float_dtype=args.float_dtype,
        max_input_length=args.max_input_length,
        bad_log=args.bad_log,
        compression=args.compression,
        recursive=args.recursive,
    )


if __name__ == "__main__":
    main()
