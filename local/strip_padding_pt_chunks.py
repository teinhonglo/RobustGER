#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""strip_padding_pt_chunks.py

Post-process existing *.pt chunk files generated by `generate_robust_hp_fast.py`
to remove Whisper 30s padding **from stored encoder features**.

It overwrites the original chunk files (atomic replace).

Assumptions
-----------
- Each chunk file is a `list[dict]`.
- For valid samples, each dict contains:
    id (str)
    audio_features (torch.Tensor)
    clean_audio_features (torch.Tensor)
  and other fields.
- `id` in noisy set looks like "noise-XXXX"; clean utt id is `id.replace("noise-", "")`.
- `wav.scp` format: `utt_id  /path/to.wav`

How trimming length is computed (no decode rerun)
-------------------------------------------------
We use only WAV headers (torchaudio.info) to get duration, then map to Whisper
encoder time steps.
Whisper uses 16kHz audio, hop_length=160 samples (10ms) -> max 3000 mel frames
for 30s, and encoder output length is 1500.

We compute:
  n16 = round(num_frames * 16000 / sample_rate)
  mel_len = min(ceil(n16 / 160), 3000)
  enc_len = ceil(mel_len / 2) = (mel_len + 1)//2  (cap to 1500)

Then we slice features on the time axis:
- [T, D]      -> [:enc_len]
- [1, T, D]   -> [:, :enc_len]

This yields variable-length features without 30s padding.

Usage
-----
python strip_padding_pt_chunks.py \
  --pt_dir data/dump/ihm_train_noise_snr0 \
  --noisy_wavscp dump/raw/ihm_train_sp/wav.scp \
  --clean_wavscp dump/raw/ihm_train_sp/wav.scp \
  --recursive \
  --dry_run 0

Notes
-----
- This is I/O heavy: it loads each chunk file once and queries torchaudio.info
  per utterance.
- It will add two new integer fields to each *valid* sample:
    audio_enc_len, clean_audio_enc_len
  (and mel_len variants) for downstream convenience.
"""

from __future__ import annotations

import argparse
import glob
import os
import re
import json
from typing import Dict, List, Tuple, Optional

import torch
import torchaudio
from tqdm import tqdm

try:
    import whisper  # optional fallback when torchaudio.info cannot parse wav.scp entries
except Exception:
    whisper = None


def atomic_torch_save(obj, path: str) -> None:
    tmp = path + ".tmp"
    torch.save(obj, tmp)
    os.replace(tmp, path)

def _chunk_sort_key(path: str) -> Tuple[int, str]:
    base = os.path.basename(path)
    m = re.search(r"chunk(\d+)", base)
    if m:
        return (int(m.group(1)), base)
    return (10**18, base)


def list_pt_files(pt_dir: str, recursive: bool) -> List[str]:
    pattern = "**/*.pt" if recursive else "*.pt"
    paths = glob.glob(os.path.join(pt_dir, pattern), recursive=recursive)
    paths = [p for p in paths if os.path.isfile(p)]
    paths = sorted(paths, key=_chunk_sort_key)
    if not paths:
        raise FileNotFoundError(f"No *.pt found under: {pt_dir}")
    return paths


def load_wavscp(path: str) -> Dict[str, str]:
    d: Dict[str, str] = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            parts = line.split()
            if len(parts) != 2:
                raise ValueError("Length of parts {len(parts)} is not equals to 2. {parts}")
            utt, wav = parts
            d[utt] = wav
    return d


def compute_whisper_lengths(num_frames: int, sample_rate: int) -> Tuple[int, int]:
    """Return (mel_len, enc_len) for Whisper features, capped to 30s."""
    # Convert to 16k-equivalent samples
    n16 = int(round(num_frames * 16000.0 / float(sample_rate)))
    # mel frames: hop=160
    hop = 160
    mel_len = (n16 + hop - 1) // hop  # ceil
    if mel_len > 3000:
        mel_len = 3000
    # encoder frames approx ceil(mel/2)
    enc_len = (mel_len + 1) // 2
    if enc_len > 1500:
        enc_len = 1500
    if enc_len < 1:
        enc_len = 1
    return int(mel_len), int(enc_len)


def compute_whisper_lengths_from_n16(n_samples_16k: int) -> Tuple[int, int]:
    """Compute (mel_len, enc_len) from raw 16k sample length (no header needed)."""
    hop = 160
    mel_len = (int(n_samples_16k) + hop - 1) // hop
    mel_len = min(mel_len, 3000)
    enc_len = (mel_len + 1) // 2
    enc_len = min(enc_len, 1500)
    enc_len = max(enc_len, 1)
    return int(mel_len), int(enc_len)


def slice_time(feat: torch.Tensor, enc_len: int) -> torch.Tensor:
    """Slice time axis to enc_len.

    Supports shapes:
    - [T, D]
    - [1, T, D]
    - [T]
    Leaves other shapes unchanged.
    """
    if feat.dim() == 3 and feat.size(0) == 1:
        feat = feat[:, :enc_len, :].contiguous().clone()
    if feat.dim() == 2:
        feat = feat[:enc_len, :].contiguous().clone()
    if feat.dim() == 1:
        feat = feat[:enc_len].contiguous().clone()
    return feat


def get_lengths_for_wav(wav_entry: str, fallback_full_load: bool) -> Optional[Tuple[int, int]]:
    """Return (mel_len, enc_len) for a wav.scp entry.

    - Primary: torchaudio.info(path)
    - Fallback: whisper.load_audio(entry) if enabled and whisper is available
    """
    try:
        info = torchaudio.info(wav_entry)
        return compute_whisper_lengths(info.num_frames, info.sample_rate)
    except Exception:
        if not fallback_full_load:
            return None
        if whisper is None:
            return None
        try:
            audio = whisper.load_audio(wav_entry)  # full decode
            return compute_whisper_lengths_from_n16(len(audio))
        except Exception:
            return None


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--pt_dir", required=True, help="Directory containing *.pt chunk files")
    ap.add_argument("--noisy_wavscp", required=True)
    ap.add_argument("--clean_wavscp", required=True)
    ap.add_argument("--recursive", action="store_true")
    ap.add_argument("--dry_run", type=int, default=0, help="1: do not overwrite files")
    ap.add_argument(
        "--fallback_full_load",
        type=int,
        default=1,
        help="If torchaudio.info fails, fallback to full decode via whisper.load_audio (1=yes, 0=no)",
    )
    ap.add_argument("--stats_out", default=None, help="Optional JSON path to dump summary stats")
    args = ap.parse_args()

    noisy_map = load_wavscp(args.noisy_wavscp)
    clean_map = load_wavscp(args.clean_wavscp)

    pt_files = list_pt_files(args.pt_dir, recursive=args.recursive)

    # global counters
    seen = 0
    updated = 0
    skipped_none = 0
    skipped_missing_wav = 0
    skipped_info_error = 0

    for pt_path in tqdm(pt_files):
        data = torch.load(pt_path, map_location="cpu")
        if not isinstance(data, list):
            raise TypeError(f"Expect list in {pt_path}, got {type(data)}")

        file_seen = 0
        file_updated = 0
        file_skipped = 0

        for s in data:
            file_seen += 1
            seen += 1

            if not isinstance(s, dict):
                skipped_none += 1
                file_skipped += 1
                continue

            utt = s.get("id", None)
            af = s.get("audio_features", None)
            caf = s.get("clean_audio_features", None)
            if utt is None or af is None or caf is None:
                skipped_none += 1
                file_skipped += 1
                print(f"utt is {utt} or af is {af} or caf is {caf}")
                continue

            utt = str(utt)
            clean_utt = utt.replace("noise-", "")

            noisy_wav = noisy_map.get(utt)
            clean_wav = clean_map.get(clean_utt)
            if noisy_wav is None or clean_wav is None:
                skipped_missing_wav += 1
                file_skipped += 1
                print(f"noisy_wav is {noisy_wav} or clean_wav is {clean_wav}")
                continue

            lens_n = get_lengths_for_wav(noisy_wav, bool(args.fallback_full_load))
            lens_c = get_lengths_for_wav(clean_wav, bool(args.fallback_full_load))
            if lens_n is None or lens_c is None:
                skipped_info_error += 1
                file_skipped += 1
                print(f"lens_n is {lens_n} or lens_c is {lens_c}")
                continue

            mel_n, enc_n = lens_n
            mel_c, enc_c = lens_c

            # slice features
            if enc_n != af.shape[0] or True:
                #print(f"enc_n {enc_n} is not equals to af {af.shape}")
                s["audio_features"] = slice_time(af, enc_n)
                s["clean_audio_features"] = slice_time(caf, enc_c)
                # add metadata for downstream
                s["audio_mel_len"] = mel_n
                s["audio_enc_len"] = enc_n
                s["clean_audio_mel_len"] = mel_c
                s["clean_audio_enc_len"] = enc_c

                updated += 1
                file_updated += 1
            else:
                file_skipped += 1
                continue

        print(
            f"[pt] {pt_path}  seen={file_seen} updated={file_updated} skipped={file_skipped}"
        )

        if args.dry_run == 0:
            atomic_torch_save(data, pt_path)

    summary = {
        "pt_dir": args.pt_dir,
        "noisy_wavscp": args.noisy_wavscp,
        "clean_wavscp": args.clean_wavscp,
        "seen": seen,
        "updated": updated,
        "skipped_none": skipped_none,
        "skipped_missing_wav": skipped_missing_wav,
        "skipped_info_error": skipped_info_error,
    }

    print("\n==== Summary ====")
    for k, v in summary.items():
        print(f"{k}: {v}")

    if args.stats_out:
        with open(args.stats_out, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    main()
